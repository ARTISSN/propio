
lora:
  r: 16
  alpha: 32
  dropout: 0.1
  target_modules: ["q_proj", "v_proj"]
  
training:
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  warmup_steps: 100
  
inference:
  num_inference_steps: 50
  guidance_scale: 7.5
